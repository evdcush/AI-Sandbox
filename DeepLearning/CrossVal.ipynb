{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "import utils\n",
    "import functions as F\n",
    "import optimizers as Opt\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short script for generating the default x_train and x_test used here\n",
    "\n",
    "```\n",
    "# Check if train/test dataset has been\n",
    "##  created yet in user project\n",
    "if not os.path.exists(utils.IRIS_TRAIN):\n",
    "    iris_dataset = utils.IrisDataset()\n",
    "    _X_train, _X_test = iris_dataset.X_train, iris_dataset.X_test\n",
    "    #==== confirm evenly distributed class split\n",
    "    _, counts = np.unique(_X_test[:,-1], return_counts=True)\n",
    "    print(f'counts: {counts}')\n",
    "    assert np.all(counts == (_X_test.shape[0] // 3))\n",
    "    #==== save files\n",
    "    np.save(utils.IRIS_TRAIN, _X_train)\n",
    "    np.save(utils.IRIS_TEST, _X_test)\n",
    "    assert os.path.exists(utils.IRIS_TRAIN) # sanity check\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now you have a CV trainer:\n",
    "- Pick 4 random seeds all models train on\n",
    "- Pick your K for K folds\n",
    "- Pick vars you want to search for\n",
    "  - Channels : A LOT HERE\n",
    "    - depth\n",
    "    - kernel sizes\n",
    "  - activation?\n",
    "\n",
    "Default maybe just 1000-2000 iters, SGD, sigmoid, Softmax CE\n",
    "\n",
    "- then run in double-for loop over:\n",
    "  - RNG seeds\n",
    "  - model parameter candidates\n",
    "\n",
    "Save all results, average out rng-seed dims,\n",
    "plot, and evaluate what's next\n",
    "\n",
    "## How the kernel sizes were generated:\n",
    "A simple mix of primes, powers of 2, and some random ints\n",
    "```\n",
    "import primesieve\n",
    "kernels = primesieve.n_primes(40)[3:]  #====== Primes\n",
    "kernels.extend([2**i for i in range(2, 9)]) #===== Powers of 2\n",
    "kernels.extend(np.random.randint(50, 151, 10)) #==== Random ints\n",
    "#==== Shuffle list\n",
    "kernels = np.random.permutation(kernels)\n",
    "```\n",
    "\n",
    "* * *\n",
    "\n",
    "# Goal for this Cross Validation task: Channels\n",
    "Even with such a simple model for a simple learning task, there is a good deal dimensionality to the parameter space that makes finding the optimal values for any of them difficult. \n",
    "\n",
    "Apparent fitness for any given setting might--in actuality--be due to wholly different and uncorrelated affects than that setting. Maybe it was just a lucky seed, which is probably the most common fools-gold found in new local minima. Or maybe it was a particular choice of activation, for this subset of network layers, with that particular sequence of starting batches.\n",
    "\n",
    "So any given search requires robust, redundant evaluation, and a clearly defined set of fixed parameters and search targets.\n",
    "\n",
    "I've decided to focus on finding a performant set of network channels.\n",
    "\n",
    "The network channels determine both layer-sizes (the \"connectivity\" strength or intensity of a layer wrt its input), and the network depth. It is arguably the most complex hyperparameter for the network, considering the possible permutations alone. It also directly affects the number of *other* parameters within the network, such as dense layers.\n",
    "\n",
    "## \"Fixed\" parameters\n",
    "The current fixed set of parameters is:\n",
    " - **RNG Seeds: [3310, 99467, 27189, 77771]** Selected at random are four seeds. Seeds are our \"starting\" loop variable. Every channel, and every other fixed variable in rotation against a channel-set, will be evaluated against each RNG seed, and the final scores will be averaged over the four seeds to get a more robust or generalized metric of performance.\n",
    " \n",
    " \n",
    " - **Loss function: [Cross Entropy (with Softmax)]** Cross entropy is well suited for this task, and out of the two implementations available (the other being logistic CE with sigmoid activation), the softmax cross entropy seems to perform better.\n",
    " \n",
    " \n",
    " - **Optimizer: [SGD]**, There are only two optimizers available for use at the moment, and Adam is significantly more powerful than SGD. Anything SGD does well, Adam will do well, likely much better. So this was a simple choice.\n",
    " \n",
    " \n",
    " \n",
    " - **Activations: [`Sigmoid`, `SeLU`, `Swish`]** \n",
    "   - `sigmoid` is a classic activation and has decent, mostly consistent, performance on this task, and represents the \"logistic\" type of nonlinearities making it an ideal choice for benchmarking. \n",
    "   - `selu` was chosen for being a very different type of activation, as a representative of the rectifiers, as well as being the best performing activation on this task out of all the activation functions for the \"default\" channels [4, 64, 3]. Many times, `selu` will do well when `sigmoid` does not, so I'm hoping to exploit that phenomena during the search (ie, if both do well/poor/inverse).\n",
    "   - `swish`, the \"best discovered activation function\", was selected for being the only activation currently implemented with learnable parameters. The motivation here being to see if kernel sizes is at all correlated with `swish`'s performance as an activation, since greater layer sizes also increases `swish` connectivity. Swish does pretty well already on this task, though generally selu does better.\n",
    "\n",
    "* * *\n",
    "\n",
    "While channels are my target parameter, the search process should also give us much info on optimal values for the fixed parameters. \n",
    "\n",
    "For instance, if a certain activation function within the fixed rotation has consistently poorer performance than other activations, for the many different combinations of channels and seeds tested, then have some idea about what activations do not work well for this task. I intend to capture **all** data during the search within multi-dimensional array, so we can analyze at depth.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# CV Setup\n",
    "#==============================================================================\n",
    "# Model config\n",
    "#------------------\n",
    "CV_SEEDS = {'3310':3310, '99467': 99467, '27189': 27189, '77771': 77771}\n",
    "SEEDS = [3310, 99467, 27189, 77771]\n",
    "optimizer   = [Opt.SGD]\n",
    "objective   = [F.SoftmaxCrossEntropy]\n",
    "activations = [F.Sigmoid, F.SeLU, F.Swish]\n",
    "\n",
    "# Target variable: channels\n",
    "#--------------------------------------------------------------\n",
    "K_IN  = 4\n",
    "K_OUT = 3\n",
    "MAX_DEPTH = 8 # 9 layers total, including input-out\n",
    "MAX_SIZE = 650 # limit on sum kernels for any given channels sample\n",
    "kernels = np.array([  31,  41,  32, 151,  16,  53,  78, 157,  47, 149, 144, 121,  11,\n",
    "                     256,  13, 163,  55, 103, 128, 124,   4,  59, 131,  97,   8,  61,\n",
    "                      29, 101,  83,  17, 113, 167, 127, 101, 130,  37, 139,   7,  64,\n",
    "                      79, 109,  71,  43,  80,  67, 101,  19,  23,  74,  73, 107,  89,\n",
    "                      173, 137])\n",
    "# Dataset\n",
    "#-------------------\n",
    "#X_train = np.load(utils.IRIS_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.copy(kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_channels(kernels, num_samples, mdepth=MAX_DEPTH, \n",
    "                      msize=MAX_SIZE, replace=True):\n",
    "    \"\"\" randomly general num_samples channels from kernels \n",
    "    * Given kernel list, randomly select kernels of randomly generated depth\n",
    "    * Convert to list, insert K_IN (input channel) at 0, and K_OUT at end\n",
    "    Params\n",
    "    ------\n",
    "    kernels : ndarray.int\n",
    "        randomly generated list of eligible channel sizes from which channels\n",
    "        are drawn\n",
    "    num_samples : int\n",
    "        number of channels samples desired\n",
    "    mdepth : int, default=8\n",
    "        maximum number of hidden layers (excluding input/output);\n",
    "        possible depths range [1, mdepth]\n",
    "    msize : int, default=650\n",
    "        limit on sum-total kernels in a sample\n",
    "    replace : bool\n",
    "        whether kernels are randomly sampled with or without placement\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    channels_list : list(list(int))\n",
    "        list of channels samples, of length num_samples\n",
    "    \"\"\"\n",
    "    CHANNELS_LIST = []\n",
    "    # Helpers\n",
    "    #-----------------\n",
    "    #==== Random ops : depth, sampling\n",
    "    get_depth   = lambda: np.random.choice(MAX_DEPTH) + 1\n",
    "    get_kernels = lambda depth: np.random.choice(kernels, size=depth, replace=replace)\n",
    "    def _resample_bigboys(sample):\n",
    "        sample_sum = np.sum(sample)\n",
    "        if sample_sum <= msize: return sample\n",
    "        depth_in = len(sample)\n",
    "        while np.sum(sample) > msize:\n",
    "            sample = get_kernels(depth_in)\n",
    "        return sample\n",
    "    #==== type conversion, inserting input/output dims\n",
    "    def _format_sample(arr):\n",
    "        lst = list(arr)\n",
    "        lst.insert(0, K_IN)\n",
    "        lst.append(K_OUT)\n",
    "        return lst\n",
    "    \n",
    "    # Generate channels\n",
    "    #-----------------------\n",
    "    for _ in range(num_samples):\n",
    "        depth = get_depth()\n",
    "        candidate_sample = get_kernels(depth)\n",
    "        sample = _resample_bigboys(candidate_sample)        \n",
    "        channels = _format_sample(sample)\n",
    "        CHANNELS_LIST.append(channels)\n",
    "    return CHANNELS_LIST  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNELS = generate_channels(K, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('GEN_500_CHANNELS', CHANNELS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
