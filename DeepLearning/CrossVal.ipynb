{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "import utils\n",
    "import functions as F\n",
    "import optimizers as Opt\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal for this Cross Validation task: Channels\n",
    "Even with such a simple model for a simple learning task, there is a good deal dimensionality to the parameter space that makes finding the optimal values for any of them difficult. \n",
    "\n",
    "Apparent fitness for any given setting might--in actuality--be due to wholly different and uncorrelated affects than that setting. Maybe it was just a lucky seed, which is probably the most common fools-gold found in new local minima. Or maybe it was a particular choice of activation, for this subset of network layers, with that particular sequence of starting batches.\n",
    "\n",
    "So any given search requires robust evaluation, and a clearly defined set of fixed parameters and search targets.\n",
    "\n",
    "I've decided to focus on finding a performant set of network channels.\n",
    "\n",
    "The network channels determine both layer-sizes (the \"connectivity\" strength or intensity of a layer wrt its input), and the network depth. It is arguably the most complex hyperparameter for the network, considering the possible permutations alone. It also directly affects the number of *other* parameters within the network, such as dense layers.\n",
    "\n",
    "## \"Fixed\" parameters\n",
    "The current fixed set of parameters is:\n",
    " - **RNG Seeds: [3310, 99467, 27189, 77771]** \n",
    "   \n",
    "   Selected at random are four seeds. Seeds are our \"starting\" loop variable. Every channel, and every other fixed variable in rotation against a channel-set, will be evaluated against each RNG seed, and the final scores will be averaged over the four seeds to get a more robust or generalized metric of performance.\n",
    " \n",
    " \n",
    " - **Loss function: [Cross Entropy (with Softmax)]** \n",
    " \n",
    " Cross entropy is well suited for this task, and out of the two implementations available (the other being logistic CE with sigmoid activation), the softmax cross entropy seems to perform better.\n",
    " \n",
    " \n",
    " - **Optimizer: [SGD]** \n",
    " \n",
    " There are only two optimizers available for use at the moment, and Adam is significantly more powerful than SGD. Anything SGD does well, Adam will do well, likely much better. So this was a simple choice.\n",
    " \n",
    " \n",
    " \n",
    " - **Activations: [`Sigmoid`, `SeLU`, `Swish`]** \n",
    "   - `sigmoid` is a classic activation and has decent, mostly consistent, performance on this task, and represents the \"logistic\" type of nonlinearities making it an ideal choice for benchmarking.\n",
    "   \n",
    "   - `selu` was chosen for being a very different type of activation, as a representative of the rectifiers, as well as being the best performing activation on this task out of all the activation functions for the \"default\" channels [4, 64, 3]. Many times, `selu` will do well when `sigmoid` does not, so I'm hoping to exploit that phenomena during the search (ie, if both do well/poor/inverse).\n",
    "   \n",
    "   - `swish`, the \"best discovered activation function\", was selected for being the only activation currently implemented with learnable parameters. The motivation here being to see if kernel sizes is at all correlated with `swish`'s performance as an activation, since greater layer sizes also increases `swish` connectivity. Swish does pretty well already on this task, though generally selu does better.\n",
    "\n",
    "* * *\n",
    "\n",
    "\n",
    "### Secondary search\n",
    "While channels are my target parameter, the search process should also give us much info on optimal values for the fixed parameters. \n",
    "\n",
    "For instance, if a certain activation function within the fixed rotation has consistently poorer performance than other activations, for the many different combinations of channels and seeds tested, then have some idea about what activations do not work well for this task. \n",
    "\n",
    "I intend to capture **all** data during the search within multi-dimensional array, so we can analyze at depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# CV Setup\n",
    "#==============================================================================\n",
    "# Model config\n",
    "#------------------\n",
    "SEEDS = [3310, 99467, 27189, 77771]\n",
    "optimizer   = [Opt.SGD]\n",
    "objective   = [F.SoftmaxCrossEntropy]\n",
    "activations = [F.Sigmoid, F.SeLU, F.Swish]\n",
    "\n",
    "# Target variable: channels\n",
    "#--------------------------------------------------------------\n",
    "K_IN  = 4\n",
    "K_OUT = 3\n",
    "MAX_DEPTH = 3 # 5 total kernels in channels, including input/outputs\n",
    "MAX_SIZE = 700 # limit on sum kernels for any given channels sample\n",
    "kernels = np.array([  31,  41,  32, 151,  16,  53,  78, 157,  47, 149, 144, 121,  11,\n",
    "                     256,  13, 163,  55, 103, 128, 124,   4,  59, 131,  97,   8,  61,\n",
    "                      29, 101,  83,  17, 113, 167, 127, 101, 130,  37, 139,   7,  64,\n",
    "                      79, 109,  71,  43,  80,  67, 101,  19,  23,  74,  73, 107,  89,\n",
    "                      173, 137])\n",
    "# Dataset\n",
    "#-------------------\n",
    "#X_train = np.load(utils.IRIS_TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code used in cross-validation search\n",
    "Some of the code used for the search was only used for this purpose, and does not feature in the normal repo codebase.\n",
    "\n",
    "That code is reproduced here for reference. \n",
    "\n",
    "The **full cross-validation script is featured in the cell just below this one**.\n",
    "\n",
    "* * *\n",
    "\n",
    "### Splitting the dataset to the default x_train and x_test\n",
    "Normally, the full Iris dataset (150 samples) is loaded during training, and split into `x_train` and `x_test` sets then. \n",
    "\n",
    "So long as you are using the same random seed for splitting the dataset, you should not *need* to have two separate files for the training and test sets, considering how small the dataset is.\n",
    "\n",
    "However, for cross-validation we needed to make sure we were only working with the normal `x_train` data, so it became necessary to split the files so that only the training set would be accessed.\n",
    "```python\n",
    "# Check if train/test dataset has been\n",
    "##  created yet in user project\n",
    "if not os.path.exists(utils.IRIS_TRAIN):\n",
    "    iris_dataset = utils.IrisDataset()\n",
    "    _X_train, _X_test = iris_dataset.X_train, iris_dataset.X_test\n",
    "    #==== confirm evenly distributed class split\n",
    "    _, counts = np.unique(_X_test[:,-1], return_counts=True)\n",
    "    print(f'counts: {counts}')\n",
    "    assert np.all(counts == (_X_test.shape[0] // 3))\n",
    "    #==== save files\n",
    "    np.save(utils.IRIS_TRAIN, _X_train)\n",
    "    np.save(utils.IRIS_TEST, _X_test)\n",
    "    assert os.path.exists(utils.IRIS_TRAIN) # sanity check\n",
    "```\n",
    "\n",
    "* * *\n",
    "\n",
    "### Generating the kernel sizes used for channel search\n",
    "A non-trivial NAS task in its own right, the selection method for the candidate channel sizes used here is very simple.\n",
    "\n",
    "It is approximately 50 ints, selected from some primes, powers of 2, and random integers.\n",
    "```python\n",
    "import primesieve\n",
    "kernels = primesieve.n_primes(40)[3:]  #====== Primes\n",
    "kernels.extend([2**i for i in range(2, 9)]) #===== Powers of 2\n",
    "kernels.extend(np.random.randint(50, 151, 10)) #==== Random ints\n",
    "#==== Shuffle list\n",
    "kernels = np.random.permutation(kernels)\n",
    "```\n",
    "\n",
    "* * *\n",
    "\n",
    "## Generating the channels\n",
    "After the list of kernel sizes has been created, we create the full list of sample channels to be evaluated in cross-validation.\n",
    "\n",
    "Channel sizes are selected from random sampling into the kernels, for a randomly generated depth (length of channels list). Both depth and sum-total channel sizes is capped.\n",
    "\n",
    "```python\n",
    "def generate_channels(kernels, num_samples, mdepth=MAX_DEPTH, \n",
    "                      msize=MAX_SIZE, replace=True):\n",
    "    \"\"\" randomly general num_samples channels from kernels \n",
    "    * Given kernel list, randomly select kernels of randomly generated depth\n",
    "    * Convert to list, insert K_IN (input channel) at 0, and K_OUT at end\n",
    "    Params\n",
    "    ------\n",
    "    kernels : ndarray.int\n",
    "        randomly generated list of eligible channel sizes from which channels\n",
    "        are drawn\n",
    "    num_samples : int\n",
    "        number of channels samples desired\n",
    "    mdepth : int, default=700\n",
    "        maximum number of hidden layers (excluding input/output);\n",
    "        possible depths range [1, mdepth]\n",
    "    msize : int, default=650\n",
    "        limit on sum-total kernels in a sample\n",
    "    replace : bool\n",
    "        whether kernels are randomly sampled with or without replacement\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    channels_list : list(list(int))\n",
    "        list of channels samples, of length num_samples\n",
    "    \"\"\"\n",
    "    CHANNELS_LIST = []\n",
    "    # Helpers\n",
    "    #-----------------\n",
    "    #==== Random ops : depth, sampling\n",
    "    get_depth   = lambda: np.random.choice(MAX_DEPTH) + 1\n",
    "    get_kernels = lambda depth: np.random.choice(kernels, size=depth, replace=replace)\n",
    "    #==== resample when sum kernel sizes exceed max-size\n",
    "    def _resample_bigboys(sample):\n",
    "        sample_sum = np.sum(sample)\n",
    "        if sample_sum <= msize: return sample\n",
    "        depth_in = len(sample)\n",
    "        while np.sum(sample) > msize:\n",
    "            sample = get_kernels(depth_in)\n",
    "        return sample\n",
    "    #==== type conversion, inserting input/output dims\n",
    "    def _format_sample(arr):\n",
    "        lst = list(arr)\n",
    "        lst.insert(0, K_IN)\n",
    "        lst.append(K_OUT)\n",
    "        return lst\n",
    "    \n",
    "    # Generate channels\n",
    "    #-----------------------\n",
    "    for _ in range(num_samples):\n",
    "        depth = get_depth()\n",
    "        candidate_sample = get_kernels(depth)\n",
    "        sample = _resample_bigboys(candidate_sample)        \n",
    "        channels = _format_sample(sample)\n",
    "        CHANNELS_LIST.append(channels)\n",
    "    return CHANNELS_LIST\n",
    "\n",
    "# Channels generated from the following call:\n",
    "#-----> CHANNELS = generate_channels(K, 700)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation script\n",
    "Reproduced here from its originally separate .py script is the cross-validation code.\n",
    "\n",
    "It essentially specifies the constants and settings decribed above, loads the prepared generated data, and performs the CV loops.\n",
    "\n",
    "This style of CV does not train in epochs, but rather more typical iterations. As such, the \"K\"-fold style is only an observed property: each fold is trained on for 1500 iterations, and then evaluated against the odd-fold out that serves as the \"test\" set. \n",
    "\n",
    "New \"folds\" are randomly selected, but the split percentage is equivalent to 5-fold CV.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Cross validation for parameter search script\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "import layers\n",
    "import functions as F\n",
    "import optimizers as Opt\n",
    "\n",
    "#==================================================\n",
    "#                   CV Setup                      #\n",
    "#==================================================\n",
    "# Model config\n",
    "#------------------\n",
    "SEEDS = [3310, 99467, 27189, 77771]\n",
    "optimizer   = [Opt.SGD]\n",
    "objective   = [F.SoftmaxCrossEntropy]\n",
    "activations = [F.Sigmoid, F.SeLU, layers.Swish]\n",
    "\n",
    "# Training config\n",
    "#------------------\n",
    "NUM_ITERS = 1500\n",
    "batch_size = 6\n",
    "\n",
    "# Dataset\n",
    "#-------------------\n",
    "\"\"\"\n",
    "80/20 train/test split gives us an even 5-fold for cross val\n",
    "Per fold:\n",
    " * 24 test samples\n",
    " * 96 train samples\n",
    "\"\"\"\n",
    "_X_train = np.load(utils.IRIS_TRAIN)\n",
    "split_size = .8 # already default in dataset\n",
    "\n",
    "# Target variable: channels\n",
    "#--------------------------------------------------------------\n",
    "K_IN  = 4\n",
    "K_OUT = 3\n",
    "MAX_DEPTH = 3\n",
    "MAX_SIZE = 700 # limit on sum kernels for any given channels sample\n",
    "CHANNELS = list(np.load('GEN_700_CHANNELS.npy'))\n",
    "\n",
    "# Helpers\n",
    "#--------------------------------------------------------------\n",
    "def generate_dataset():\n",
    "    x_copy = np.copy(_X_train)\n",
    "    split_seed = np.random.choice(15000)\n",
    "    _dataset = utils.IrisDataset(x_copy, split_size, split_seed)\n",
    "    return _dataset\n",
    "\n",
    "def init_trainer(chan, act, dset, seed):\n",
    "    return utils.Trainer(chan, Opt.SGD, F.SoftmaxCrossEntropy,\n",
    "                         act, dataset=dset, steps=NUM_ITERS,\n",
    "                         batch_size=batch_size, rng_seed=seed)\n",
    "\n",
    "# Loss tracker\n",
    "#--------------------------------------------------------------\n",
    "cv_dims = (len(SEEDS), len(activations), len(CHANNELS), NUM_ITERS, 2)\n",
    "cv_test_dims = cv_dims[:-2] + (24, 2)\n",
    "#==== collections\n",
    "CV_TRAIN_LOSS = np.zeros(cv_dims,      np.float32)\n",
    "CV_TEST_LOSS  = np.zeros(cv_test_dims, np.float32)\n",
    "\n",
    "\n",
    "#==================================================\n",
    "#                    CV Train                     #\n",
    "#==================================================\n",
    "\n",
    "for idx_seed, seed in enumerate(SEEDS):\n",
    "    for idx_act, act in enumerate(activations):\n",
    "        dataset = generate_dataset()\n",
    "        for idx_chan, channels in enumerate(CHANNELS):\n",
    "            # copy data for safety\n",
    "            dataset.X_train = np.copy(dataset.X_train)\n",
    "            dataset.X_test  = np.copy(dataset.X_test)\n",
    "\n",
    "            # make trainer and train\n",
    "            trainer = init_trainer(channels, act, dataset, seed)\n",
    "            trainer()\n",
    "\n",
    "            # Store loss\n",
    "            lh_train, lh_test = trainer.get_loss_histories()\n",
    "            CV_TRAIN_LOSS[idx_seed, idx_act, idx_chan] = lh_train\n",
    "            CV_TEST_LOSS[ idx_seed, idx_act, idx_chan] = lh_test\n",
    "\n",
    "# Save results\n",
    "#--------------------------------------------------------------\n",
    "np.save('CV_train_loss', CV_TRAIN_LOSS) # WARNING: large file!\n",
    "np.save('CV_test_loss',  CV_TEST_LOSS)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
