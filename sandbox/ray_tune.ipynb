{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "communist-bobby",
   "metadata": {},
   "source": [
    "**Ray Tune**\n",
    "\n",
    "This notebook is from the PyTorch tutorial on [Hyperparameter Tuning with Ray](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "academic-applicant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-chess",
   "metadata": {},
   "source": [
    "## Data Loaders\n",
    "Wrap the data loaders in their own function and pass a global data directory.\n",
    "This way we can share a data directory between different trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "impressive-printing",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/evan/.Data/cifar10'\n",
    "\n",
    "def load_data(data_dir=DATA_DIR):\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=True, download=True, transform=transform)\n",
    "    \n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=False, download=True, transform=transform)\n",
    "    \n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-value",
   "metadata": {},
   "source": [
    "## Configurable Neural Network\n",
    "We can only tune those parameters that are configurable.\n",
    "In this example, we can specify the layer sizes of the fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "essential-small",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, l1=120, l2=84):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool  = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16 * 5 * 5, l1)\n",
    "        self.fc2   = nn.Linear(l1, l2)\n",
    "        self.fc3   = nn.Linear(l2, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.pool(F.relu(self.conv1(x)))\n",
    "        h = self.pool(F.relu(self.conv2(h)))\n",
    "        h = h.view(-1, 16 * 5 * 5)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        h = self.fc3(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-glenn",
   "metadata": {},
   "source": [
    "## Train Function\n",
    "We wrap an entire training script into a function that accepts a config of tunable hyperparameters, and optionally some relevant directories for checkpointing and data.\n",
    "\n",
    "The training function is fairly standard/vanilla except that **we communicate our validation metrics to Ray Tune**. Ray Tune uses these metrics to decide which hyperparameter configuration lead to the best results. These metrics can also be used to **stop bad performing trials early** in order to avoid wasting resources on those trials.\n",
    "\n",
    "The **checkpoint saving** is optional, however, it is necessary if we wanted to use advanced schedulers like [**Population Based Training**](https://docs.ray.io/en/master/tune/tutorials/tune-advanced-tutorial.html). Also, by saving the checkpoint, we can later load the trained models and validate them on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "developed-plate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cifar(cfg, checkpoint_dir=None, data_dir=None):\n",
    "    \"\"\" A complete training script, wrapped in a function.\n",
    "    Parameters from our config, ``cfg``, are tunable.\n",
    "    \"\"\"\n",
    "    # Make network, with FC layer sizes from config.\n",
    "    net = Net(cfg['l1'], cfg['l2'])\n",
    "    \n",
    "    # Set device.\n",
    "    ## Defaults to CPU, but will use GPU(s) if they are available.\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "    \n",
    "    # Optimizer and objective.\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=cfg['lr'], momentum=0.9)\n",
    "    \n",
    "    # Load checkpoints if available.\n",
    "    if checkpoint_dir:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint')\n",
    "        model_state, optimizer_state = torch.load(checkpoint_path)\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "    \n",
    "    # Load data.\n",
    "    trainset, testset = load_data(data_dir)\n",
    "    \n",
    "    # Split train into 80/20 train and validation sets.\n",
    "    N = len(trainset)\n",
    "    test_abs = int(N * 0.8)\n",
    "    train_subset, val_subset = random_split(trainset, [test_abs, N - test_abs])\n",
    "    \n",
    "    # Make dataloaders.\n",
    "    batch_size = int(cfg['batch_size'])\n",
    "    data_loader = partial(DataLoader, batch_size=batch_size, \n",
    "                          shuffle=True, num_workers=8)\n",
    "    train_loader = data_loader(train_subset)\n",
    "    val_loader   = data_loader(val_subset)\n",
    "    \n",
    "    # Training and validation helper functions.\n",
    "    def _train(epoch, epoch_steps):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # Get the inputs; data : list[inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients.\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward --> Backward --> Optimize.\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print statistics.\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:\n",
    "                print(f\"[{epoch + 1}, {i + 1}] loss: \"\n",
    "                      f\"{running_loss / epoch_steps: .3f}\")\n",
    "                running_loss = 0.0\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _validation():\n",
    "        val_loss  = 0.0\n",
    "        val_steps = 0\n",
    "        total   = 0\n",
    "        correct = 0\n",
    "        for data in val_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward & predictions\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total   += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss  += loss.cpu().numpy()\n",
    "            val_steps += 1\n",
    "        \n",
    "        loss = val_loss / val_steps\n",
    "        accuracy = correct / total\n",
    "        return loss, accuracy\n",
    "    \n",
    "    # Checkpoint helper function.\n",
    "    def _checkpoint(epoch):\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, 'checkpoint')\n",
    "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "            \n",
    "    # Train loop.\n",
    "    for epoch in range(10):\n",
    "        epoch_steps = 0\n",
    "        \n",
    "        # Training and validation.\n",
    "        _train(epoch, epoch_steps)\n",
    "        val_loss, val_acc = _validation()\n",
    "        \n",
    "        # Save checkpoint and report current performance.\n",
    "        #_checkpoint(epoch)\n",
    "        #tune.report(loss=val_loss, accuracy=val_acc)\n",
    "    \n",
    "    print(\"\\nFinished training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-currency",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hairy-veteran",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_accuracy(net, device='cpu'):\n",
    "    # Load the data.\n",
    "    _, testset = load_data()\n",
    "    test_loader = DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Test loop.\n",
    "    correct = total = 0\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total   += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    return correct / total    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-coast",
   "metadata": {},
   "source": [
    "## Configuring the Search Space\n",
    "Lastly, we need to define Ray Tune's search space.\n",
    "\n",
    "The `tune.sample_from()` function makes it possible to define your own sample methods to obtain hyperparameters.\n",
    "\n",
    "In our example, the `l1` and `l2` parameters should be powers of 2 between 4 and 256 (so either 4, 8, 16, 32, 128, 256). The learning rate, `lr`, should be uniformly sampled between 0.0001 and 0.1. Lastly, the batch size is a choice between 2, 4, 8, and 16.\n",
    "\n",
    "At each trial, Ray Tune will now randomly sample a combination of parameters from these search spaces. It will then train a number of models in parallel and find the best performing one among these.\n",
    "\n",
    "We also use the `ASHAScheduler`, which will terminate bad performing trials early.\n",
    "\n",
    "We wrap the `train_cifar` function with `functools.partial` to set the constant `data_dir` arg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "spectacular-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'l1': tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n",
    "    'l2': tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n",
    "    'lr': tune.loguniform(1e-4, 1e-1),\n",
    "    'batch_size': tune.choice([2, 4, 8, 16]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-bishop",
   "metadata": {},
   "source": [
    "We can also tell Ray Tune what resources should be available for each trial.\n",
    "\n",
    "You can specify the number of CPUs, which are then available e.g. to increase the `num_workers` of the torch `DataLoader` instances. The selected number of GPUs are made visible to torch in each trial. Trials do not need to have access to GPUs that haven't been requested for them, so you don't have to care about two trials using the same set of resources.\n",
    "\n",
    "Here we can also specify **fractional GPUs**, so something like `gpus_per_trial=0.5` is completely valid. The trials will then share GPUs among each other. You just have to make sure that the models still fit in the GPU memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-inquiry",
   "metadata": {},
   "source": [
    "After training the models, we will find the best performing one and load the trained network from the checkpoint file. We then obtain the test set accuracy and report everything by printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "spread-threat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=1):\n",
    "    data_dir = DATA_DIR\n",
    "    load_data(data_dir) # Run here so we can download, if need be.\n",
    "    \n",
    "    scheduler = ASHAScheduler(\n",
    "        metric='loss',\n",
    "        mode='min',\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    \n",
    "    reporter = CLIReporter(\n",
    "        # parameter_columns=['l1', 'l2', 'lr', 'batch_size'],\n",
    "        metric_columns=['loss', 'accuracy', 'training_iteration'])\n",
    "    \n",
    "    # Run the search.\n",
    "    result = tune.run(\n",
    "        partial(train_cifar, data_dir=DATA_DIR),\n",
    "        resources_per_trial={'cpu': 1, 'gpu': gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter,)\n",
    "    \n",
    "    # Print results.\n",
    "    best_trial = result.get_best_trial('loss', 'min', 'last')\n",
    "    print(f\"Best trial config: {best_trial.config}\")\n",
    "    print(\"Best trial final validation loss: \"\n",
    "          f\"{best_trial.last_result['loss']}\")\n",
    "    print(\"Best trial final validation accuracy: \"\n",
    "          f\"{best_trial.last_result['accuracy']}\")\n",
    "    \n",
    "    # Get best model's test accuracy.\n",
    "    best_trained_model = Net(best_trial.config['l1'], best_trial.config['l2'])\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "    \n",
    "    best_checkpoint_dir = os.path.join(best_trial.checkpoint.value, 'checkpoint')\n",
    "    model_state, optimizer_state = torch.load(best_checkpoint_dir)\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "    \n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(f\"Best trial test set accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "normal-annotation",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 06:46:52,706\tINFO services.py:1172 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2021-03-17 06:46:54,179\tWARNING experiment.py:291 -- No name detected on trainable. Using DEFAULT.\n",
      "2021-03-17 06:46:54,180\tINFO registry.py:64 -- Detected unknown callable for trainable. Converting to class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 6.6/23.4 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 1/8 CPUs, 1/1 GPUs, 0.0/11.04 GiB heap, 0.0/3.81 GiB objects (0/1.0 accelerator_type:GTX)\n",
      "Result logdir: /home/evan/ray_results/DEFAULT_2021-03-17_06-46-54\n",
      "Number of trials: 1/10 (1 RUNNING)\n",
      "+---------------------+----------+-------+--------------+------+------+------------+\n",
      "| Trial name          | status   | loc   |   batch_size |   l1 |   l2 |         lr |\n",
      "|---------------------+----------+-------+--------------+------+------+------------|\n",
      "| DEFAULT_1fa08_00000 | RUNNING  |       |            8 |    4 |  128 | 0.00246176 |\n",
      "+---------------------+----------+-------+--------------+------+------+------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=38440)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=38440)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=38440)\u001b[0m [1, 2000] loss:  2.307\n",
      "\u001b[2m\u001b[36m(pid=38440)\u001b[0m [1, 4000] loss:  1.117\n",
      "\u001b[2m\u001b[36m(pid=38440)\u001b[0m [2, 2000] loss:  1.811\n",
      "\u001b[2m\u001b[36m(pid=38440)\u001b[0m [2, 4000] loss:  0.848\n",
      "\u001b[2m\u001b[36m(pid=38440)\u001b[0m [3, 2000] loss:  1.532\n",
      "\u001b[2m\u001b[36m(pid=38440)\u001b[0m [3, 4000] loss:  0.742\n",
      "\u001b[2m\u001b[36m(pid=38440)\u001b[0m [4, 2000] loss:  1.440\n",
      "\u001b[2m\u001b[36m(pid=38440)\u001b[0m [4, 4000] loss:  0.705\n",
      "\u001b[2m\u001b[36m(pid=38440)\u001b[0m [5, 2000] loss:  1.393\n",
      "\u001b[2m\u001b[36m(pid=38440)\u001b[0m [5, 4000] loss:  0.693\n",
      "\u001b[2m\u001b[36m(pid=38440)\u001b[0m [6, 2000] loss:  1.362\n",
      "\u001b[2m\u001b[36m(pid=38440)\u001b[0m [6, 4000] loss:  0.691\n",
      "\u001b[2m\u001b[36m(pid=38440)\u001b[0m [7, 2000] loss:  1.357\n",
      "\u001b[2m\u001b[36m(pid=38440)\u001b[0m [7, 4000] loss:  0.672\n",
      "\u001b[2m\u001b[36m(pid=38440)\u001b[0m [8, 2000] loss:  1.317\n",
      "\u001b[2m\u001b[36m(pid=38440)\u001b[0m [8, 4000] loss:  0.670\n",
      "\u001b[2m\u001b[36m(pid=38440)\u001b[0m [9, 2000] loss:  1.325\n",
      "\u001b[2m\u001b[36m(pid=38440)\u001b[0m [9, 4000] loss:  0.659\n",
      "\u001b[2m\u001b[36m(pid=38440)\u001b[0m [10, 2000] loss:  1.304\n",
      "\u001b[2m\u001b[36m(pid=38440)\u001b[0m [10, 4000] loss:  0.663\n",
      "Trial DEFAULT_1fa08_00000 completed. Last result: \n",
      "== Status ==\n",
      "Memory usage on this node: 8.5/23.4 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/11.04 GiB heap, 0.0/3.81 GiB objects (0/1.0 accelerator_type:GTX)\n",
      "Result logdir: /home/evan/ray_results/DEFAULT_2021-03-17_06-46-54\n",
      "Number of trials: 2/10 (1 PENDING, 1 TERMINATED)\n",
      "+---------------------+------------+-------+--------------+------+------+-------------+\n",
      "| Trial name          | status     | loc   |   batch_size |   l1 |   l2 |          lr |\n",
      "|---------------------+------------+-------+--------------+------+------+-------------|\n",
      "| DEFAULT_1fa08_00001 | PENDING    |       |            2 |   64 |    4 | 0.000623291 |\n",
      "| DEFAULT_1fa08_00000 | TERMINATED |       |            8 |    4 |  128 | 0.00246176  |\n",
      "+---------------------+------------+-------+--------------+------+------+-------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=38440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=38440)\u001b[0m Finished training!\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [1, 2000] loss:  2.309\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [1, 4000] loss:  1.105\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [1, 6000] loss:  0.659\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [1, 8000] loss:  0.473\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [1, 10000] loss:  0.360\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [1, 12000] loss:  0.292\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [1, 14000] loss:  0.243\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [1, 16000] loss:  0.206\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [1, 18000] loss:  0.183\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [1, 20000] loss:  0.160\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [2, 2000] loss:  1.555\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [2, 4000] loss:  0.763\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [2, 6000] loss:  0.493\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [2, 8000] loss:  0.373\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [2, 10000] loss:  0.299\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [2, 12000] loss:  0.246\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [2, 14000] loss:  0.214\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [2, 16000] loss:  0.180\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [2, 18000] loss:  0.158\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [2, 20000] loss:  0.143\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [3, 2000] loss:  1.380\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [3, 4000] loss:  0.682\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [3, 6000] loss:  0.457\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [3, 8000] loss:  0.345\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [3, 10000] loss:  0.272\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [3, 12000] loss:  0.224\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [3, 14000] loss:  0.196\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [3, 16000] loss:  0.168\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [3, 18000] loss:  0.149\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [3, 20000] loss:  0.135\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [4, 2000] loss:  1.270\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [4, 4000] loss:  0.643\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [4, 6000] loss:  0.431\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [4, 8000] loss:  0.321\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [4, 10000] loss:  0.263\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [4, 12000] loss:  0.214\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [4, 14000] loss:  0.183\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [4, 16000] loss:  0.162\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [4, 18000] loss:  0.144\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [4, 20000] loss:  0.126\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [5, 2000] loss:  1.192\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [5, 4000] loss:  0.601\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [5, 6000] loss:  0.410\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [5, 8000] loss:  0.309\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [5, 10000] loss:  0.247\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [5, 12000] loss:  0.208\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [5, 14000] loss:  0.178\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [5, 16000] loss:  0.157\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [5, 18000] loss:  0.134\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [5, 20000] loss:  0.121\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [6, 2000] loss:  1.153\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [6, 4000] loss:  0.579\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [6, 6000] loss:  0.388\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [6, 8000] loss:  0.301\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [6, 10000] loss:  0.238\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [6, 12000] loss:  0.197\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [6, 14000] loss:  0.172\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [6, 16000] loss:  0.150\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [6, 18000] loss:  0.132\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [6, 20000] loss:  0.118\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [7, 2000] loss:  1.115\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [7, 4000] loss:  0.553\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [7, 6000] loss:  0.378\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [7, 8000] loss:  0.284\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [7, 10000] loss:  0.229\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [7, 12000] loss:  0.194\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [7, 14000] loss:  0.166\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [7, 16000] loss:  0.144\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [7, 18000] loss:  0.129\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [7, 20000] loss:  0.117\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [8, 2000] loss:  1.065\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [8, 4000] loss:  0.541\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [8, 6000] loss:  0.369\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [8, 8000] loss:  0.280\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [8, 10000] loss:  0.229\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [8, 12000] loss:  0.189\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [8, 14000] loss:  0.164\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [8, 16000] loss:  0.139\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [8, 18000] loss:  0.130\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [8, 20000] loss:  0.112\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [9, 2000] loss:  1.050\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [9, 4000] loss:  0.532\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [9, 6000] loss:  0.355\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [9, 8000] loss:  0.269\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [9, 10000] loss:  0.215\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [9, 12000] loss:  0.186\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [9, 14000] loss:  0.155\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [9, 16000] loss:  0.137\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [9, 18000] loss:  0.125\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [9, 20000] loss:  0.113\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [10, 2000] loss:  1.020\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [10, 4000] loss:  0.519\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [10, 6000] loss:  0.348\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [10, 8000] loss:  0.266\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [10, 10000] loss:  0.216\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [10, 12000] loss:  0.180\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [10, 14000] loss:  0.156\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [10, 16000] loss:  0.135\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [10, 18000] loss:  0.124\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m [10, 20000] loss:  0.110\n",
      "Trial DEFAULT_1fa08_00001 completed. Last result: \n",
      "== Status ==\n",
      "Memory usage on this node: 8.8/23.4 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/11.04 GiB heap, 0.0/3.81 GiB objects (0/1.0 accelerator_type:GTX)\n",
      "Result logdir: /home/evan/ray_results/DEFAULT_2021-03-17_06-46-54\n",
      "Number of trials: 3/10 (1 PENDING, 2 TERMINATED)\n",
      "+---------------------+------------+-------+--------------+------+------+-------------+\n",
      "| Trial name          | status     | loc   |   batch_size |   l1 |   l2 |          lr |\n",
      "|---------------------+------------+-------+--------------+------+------+-------------|\n",
      "| DEFAULT_1fa08_00002 | PENDING    |       |            2 |  128 |    8 | 0.000215738 |\n",
      "| DEFAULT_1fa08_00000 | TERMINATED |       |            8 |    4 |  128 | 0.00246176  |\n",
      "| DEFAULT_1fa08_00001 | TERMINATED |       |            2 |   64 |    4 | 0.000623291 |\n",
      "+---------------------+------------+-------+--------------+------+------+-------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=38438)\u001b[0m Finished training!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [1, 2000] loss:  2.309\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [1, 4000] loss:  1.152\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [1, 6000] loss:  0.767\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [1, 8000] loss:  0.573\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [1, 10000] loss:  0.452\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [1, 12000] loss:  0.366\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [1, 14000] loss:  0.303\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [1, 16000] loss:  0.254\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [1, 18000] loss:  0.215\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [1, 20000] loss:  0.188\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [2, 2000] loss:  1.810\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [2, 4000] loss:  0.877\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [2, 6000] loss:  0.581\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [2, 8000] loss:  0.428\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [2, 10000] loss:  0.336\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [2, 12000] loss:  0.278\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [2, 14000] loss:  0.235\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [2, 16000] loss:  0.204\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [2, 18000] loss:  0.177\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [2, 20000] loss:  0.159\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [3, 2000] loss:  1.558\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [3, 4000] loss:  0.766\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [3, 6000] loss:  0.512\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [3, 8000] loss:  0.376\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [3, 10000] loss:  0.304\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [3, 12000] loss:  0.248\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [3, 14000] loss:  0.206\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [3, 16000] loss:  0.180\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [3, 18000] loss:  0.163\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [3, 20000] loss:  0.143\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [4, 2000] loss:  1.424\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [4, 4000] loss:  0.702\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [4, 6000] loss:  0.467\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [4, 8000] loss:  0.340\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [4, 10000] loss:  0.270\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [4, 12000] loss:  0.225\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [4, 14000] loss:  0.188\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [4, 16000] loss:  0.168\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [4, 18000] loss:  0.149\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [4, 20000] loss:  0.132\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [5, 2000] loss:  1.280\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [5, 4000] loss:  0.622\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [5, 6000] loss:  0.434\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [5, 8000] loss:  0.321\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [5, 10000] loss:  0.255\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [5, 12000] loss:  0.214\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [5, 14000] loss:  0.179\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [5, 16000] loss:  0.156\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [5, 18000] loss:  0.140\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [5, 20000] loss:  0.125\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [6, 2000] loss:  1.217\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [6, 4000] loss:  0.605\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [6, 6000] loss:  0.393\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [6, 8000] loss:  0.303\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [6, 10000] loss:  0.231\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [6, 12000] loss:  0.197\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [6, 14000] loss:  0.169\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [6, 16000] loss:  0.150\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [6, 18000] loss:  0.130\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [6, 20000] loss:  0.116\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [7, 2000] loss:  1.068\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [7, 4000] loss:  0.580\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [7, 6000] loss:  0.370\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [7, 8000] loss:  0.273\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [7, 10000] loss:  0.223\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [7, 12000] loss:  0.184\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [7, 14000] loss:  0.157\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [7, 16000] loss:  0.137\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [7, 18000] loss:  0.124\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [7, 20000] loss:  0.111\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [8, 2000] loss:  1.008\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [8, 4000] loss:  0.517\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [8, 6000] loss:  0.359\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [8, 8000] loss:  0.251\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [8, 10000] loss:  0.209\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [8, 12000] loss:  0.175\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [8, 14000] loss:  0.153\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [8, 16000] loss:  0.130\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [8, 18000] loss:  0.114\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [8, 20000] loss:  0.105\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [9, 2000] loss:  0.989\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [9, 4000] loss:  0.478\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [9, 6000] loss:  0.323\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [9, 8000] loss:  0.252\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [9, 10000] loss:  0.195\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [9, 12000] loss:  0.162\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [9, 14000] loss:  0.148\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [9, 16000] loss:  0.123\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [9, 18000] loss:  0.110\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [9, 20000] loss:  0.098\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [10, 2000] loss:  0.894\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [10, 4000] loss:  0.463\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [10, 6000] loss:  0.305\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [10, 8000] loss:  0.226\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [10, 10000] loss:  0.194\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [10, 12000] loss:  0.161\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [10, 14000] loss:  0.134\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [10, 16000] loss:  0.117\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [10, 18000] loss:  0.104\n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m [10, 20000] loss:  0.095\n",
      "Trial DEFAULT_1fa08_00002 completed. Last result: \n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=38439)\u001b[0m Finished training!\n",
      "== Status ==\n",
      "Memory usage on this node: 8.8/23.4 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/11.04 GiB heap, 0.0/3.81 GiB objects (0/1.0 accelerator_type:GTX)\n",
      "Result logdir: /home/evan/ray_results/DEFAULT_2021-03-17_06-46-54\n",
      "Number of trials: 4/10 (1 PENDING, 3 TERMINATED)\n",
      "+---------------------+------------+-------+--------------+------+------+-------------+\n",
      "| Trial name          | status     | loc   |   batch_size |   l1 |   l2 |          lr |\n",
      "|---------------------+------------+-------+--------------+------+------+-------------|\n",
      "| DEFAULT_1fa08_00003 | PENDING    |       |           16 |   64 |   32 | 0.0148784   |\n",
      "| DEFAULT_1fa08_00000 | TERMINATED |       |            8 |    4 |  128 | 0.00246176  |\n",
      "| DEFAULT_1fa08_00001 | TERMINATED |       |            2 |   64 |    4 | 0.000623291 |\n",
      "| DEFAULT_1fa08_00002 | TERMINATED |       |            2 |  128 |    8 | 0.000215738 |\n",
      "+---------------------+------------+-------+--------------+------+------+-------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=38441)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=38441)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=38441)\u001b[0m [1, 2000] loss:  1.789\n",
      "\u001b[2m\u001b[36m(pid=38441)\u001b[0m [2, 2000] loss:  1.543\n",
      "\u001b[2m\u001b[36m(pid=38441)\u001b[0m [3, 2000] loss:  1.496\n",
      "\u001b[2m\u001b[36m(pid=38441)\u001b[0m [4, 2000] loss:  1.471\n",
      "\u001b[2m\u001b[36m(pid=38441)\u001b[0m [5, 2000] loss:  1.444\n",
      "\u001b[2m\u001b[36m(pid=38441)\u001b[0m [6, 2000] loss:  1.449\n",
      "\u001b[2m\u001b[36m(pid=38441)\u001b[0m [7, 2000] loss:  1.458\n",
      "\u001b[2m\u001b[36m(pid=38441)\u001b[0m [8, 2000] loss:  1.442\n",
      "\u001b[2m\u001b[36m(pid=38441)\u001b[0m [9, 2000] loss:  1.450\n",
      "\u001b[2m\u001b[36m(pid=38441)\u001b[0m [10, 2000] loss:  1.443\n",
      "Trial DEFAULT_1fa08_00003 completed. Last result: \n",
      "== Status ==\n",
      "Memory usage on this node: 8.9/23.4 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/11.04 GiB heap, 0.0/3.81 GiB objects (0/1.0 accelerator_type:GTX)\n",
      "Result logdir: /home/evan/ray_results/DEFAULT_2021-03-17_06-46-54\n",
      "Number of trials: 5/10 (1 PENDING, 4 TERMINATED)\n",
      "+---------------------+------------+-------+--------------+------+------+-------------+\n",
      "| Trial name          | status     | loc   |   batch_size |   l1 |   l2 |          lr |\n",
      "|---------------------+------------+-------+--------------+------+------+-------------|\n",
      "| DEFAULT_1fa08_00004 | PENDING    |       |           16 |  128 |   16 | 0.00594788  |\n",
      "| DEFAULT_1fa08_00000 | TERMINATED |       |            8 |    4 |  128 | 0.00246176  |\n",
      "| DEFAULT_1fa08_00001 | TERMINATED |       |            2 |   64 |    4 | 0.000623291 |\n",
      "| DEFAULT_1fa08_00002 | TERMINATED |       |            2 |  128 |    8 | 0.000215738 |\n",
      "| DEFAULT_1fa08_00003 | TERMINATED |       |           16 |   64 |   32 | 0.0148784   |\n",
      "+---------------------+------------+-------+--------------+------+------+-------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=38441)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=38441)\u001b[0m Finished training!\n",
      "\u001b[2m\u001b[36m(pid=38436)\u001b[0m Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=38436)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=38436)\u001b[0m [1, 2000] loss:  1.846\n",
      "\u001b[2m\u001b[36m(pid=38436)\u001b[0m [2, 2000] loss:  1.431\n",
      "\u001b[2m\u001b[36m(pid=38436)\u001b[0m [3, 2000] loss:  1.283\n",
      "\u001b[2m\u001b[36m(pid=38436)\u001b[0m [4, 2000] loss:  1.191\n",
      "\u001b[2m\u001b[36m(pid=38436)\u001b[0m [5, 2000] loss:  1.127\n",
      "\u001b[2m\u001b[36m(pid=38436)\u001b[0m [6, 2000] loss:  1.073\n",
      "\u001b[2m\u001b[36m(pid=38436)\u001b[0m [7, 2000] loss:  1.023\n",
      "\u001b[2m\u001b[36m(pid=38436)\u001b[0m [8, 2000] loss:  0.979\n",
      "\u001b[2m\u001b[36m(pid=38436)\u001b[0m [9, 2000] loss:  0.945\n",
      "\u001b[2m\u001b[36m(pid=38436)\u001b[0m [10, 2000] loss:  0.927\n",
      "Trial DEFAULT_1fa08_00004 completed. Last result: \n",
      "== Status ==\n",
      "Memory usage on this node: 8.8/23.4 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/11.04 GiB heap, 0.0/3.81 GiB objects (0/1.0 accelerator_type:GTX)\n",
      "Result logdir: /home/evan/ray_results/DEFAULT_2021-03-17_06-46-54\n",
      "Number of trials: 6/10 (1 PENDING, 5 TERMINATED)\n",
      "+---------------------+------------+-------+--------------+------+------+-------------+\n",
      "| Trial name          | status     | loc   |   batch_size |   l1 |   l2 |          lr |\n",
      "|---------------------+------------+-------+--------------+------+------+-------------|\n",
      "| DEFAULT_1fa08_00005 | PENDING    |       |           16 |  128 |   32 | 0.000443173 |\n",
      "| DEFAULT_1fa08_00000 | TERMINATED |       |            8 |    4 |  128 | 0.00246176  |\n",
      "| DEFAULT_1fa08_00001 | TERMINATED |       |            2 |   64 |    4 | 0.000623291 |\n",
      "| DEFAULT_1fa08_00002 | TERMINATED |       |            2 |  128 |    8 | 0.000215738 |\n",
      "| DEFAULT_1fa08_00003 | TERMINATED |       |           16 |   64 |   32 | 0.0148784   |\n",
      "| DEFAULT_1fa08_00004 | TERMINATED |       |           16 |  128 |   16 | 0.00594788  |\n",
      "+---------------------+------------+-------+--------------+------+------+-------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=38436)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=38436)\u001b[0m Finished training!\n",
      "\u001b[2m\u001b[36m(pid=38437)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=38437)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=38437)\u001b[0m [1, 2000] loss:  2.300\n",
      "\u001b[2m\u001b[36m(pid=38437)\u001b[0m [2, 2000] loss:  2.053\n",
      "\u001b[2m\u001b[36m(pid=38437)\u001b[0m [3, 2000] loss:  1.726\n",
      "\u001b[2m\u001b[36m(pid=38437)\u001b[0m [4, 2000] loss:  1.594\n",
      "\u001b[2m\u001b[36m(pid=38437)\u001b[0m [5, 2000] loss:  1.519\n",
      "\u001b[2m\u001b[36m(pid=38437)\u001b[0m [6, 2000] loss:  1.446\n",
      "\u001b[2m\u001b[36m(pid=38437)\u001b[0m [7, 2000] loss:  1.384\n",
      "\u001b[2m\u001b[36m(pid=38437)\u001b[0m [8, 2000] loss:  1.331\n",
      "\u001b[2m\u001b[36m(pid=38437)\u001b[0m [9, 2000] loss:  1.286\n",
      "\u001b[2m\u001b[36m(pid=38437)\u001b[0m [10, 2000] loss:  1.241\n",
      "Trial DEFAULT_1fa08_00005 completed. Last result: \n",
      "== Status ==\n",
      "Memory usage on this node: 8.8/23.4 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/11.04 GiB heap, 0.0/3.81 GiB objects (0/1.0 accelerator_type:GTX)\n",
      "Result logdir: /home/evan/ray_results/DEFAULT_2021-03-17_06-46-54\n",
      "Number of trials: 7/10 (1 PENDING, 6 TERMINATED)\n",
      "+---------------------+------------+-------+--------------+------+------+-------------+\n",
      "| Trial name          | status     | loc   |   batch_size |   l1 |   l2 |          lr |\n",
      "|---------------------+------------+-------+--------------+------+------+-------------|\n",
      "| DEFAULT_1fa08_00006 | PENDING    |       |            2 |    8 |   16 | 0.0111494   |\n",
      "| DEFAULT_1fa08_00000 | TERMINATED |       |            8 |    4 |  128 | 0.00246176  |\n",
      "| DEFAULT_1fa08_00001 | TERMINATED |       |            2 |   64 |    4 | 0.000623291 |\n",
      "| DEFAULT_1fa08_00002 | TERMINATED |       |            2 |  128 |    8 | 0.000215738 |\n",
      "| DEFAULT_1fa08_00003 | TERMINATED |       |           16 |   64 |   32 | 0.0148784   |\n",
      "| DEFAULT_1fa08_00004 | TERMINATED |       |           16 |  128 |   16 | 0.00594788  |\n",
      "| DEFAULT_1fa08_00005 | TERMINATED |       |           16 |  128 |   32 | 0.000443173 |\n",
      "+---------------------+------------+-------+--------------+------+------+-------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=38437)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=38437)\u001b[0m Finished training!\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [1, 2000] loss:  2.266\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [1, 4000] loss:  1.114\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [1, 6000] loss:  0.766\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [1, 8000] loss:  0.579\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [1, 10000] loss:  0.463\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [1, 12000] loss:  0.386\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [1, 14000] loss:  0.331\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [1, 16000] loss:  0.289\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [1, 18000] loss:  0.257\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [1, 20000] loss:  0.232\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [2, 2000] loss:  2.317\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [2, 4000] loss:  1.157\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [2, 6000] loss:  0.771\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [2, 8000] loss:  0.579\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [2, 10000] loss:  0.463\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [2, 12000] loss:  0.386\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [2, 14000] loss:  0.331\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [2, 16000] loss:  0.289\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [2, 18000] loss:  0.257\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [2, 20000] loss:  0.232\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [3, 2000] loss:  2.315\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [3, 4000] loss:  1.158\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [3, 6000] loss:  0.771\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [3, 8000] loss:  0.579\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [3, 10000] loss:  0.463\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [3, 12000] loss:  0.386\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [3, 14000] loss:  0.331\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [3, 16000] loss:  0.289\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [3, 18000] loss:  0.257\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [3, 20000] loss:  0.232\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [4, 2000] loss:  2.315\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [4, 4000] loss:  1.157\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [4, 6000] loss:  0.771\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [4, 8000] loss:  0.579\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [4, 10000] loss:  0.463\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [4, 12000] loss:  0.386\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [4, 14000] loss:  0.331\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [4, 16000] loss:  0.289\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [4, 18000] loss:  0.257\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [4, 20000] loss:  0.231\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [5, 2000] loss:  2.318\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [5, 4000] loss:  1.157\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [5, 6000] loss:  0.772\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [5, 8000] loss:  0.578\n",
      "\u001b[2m\u001b[36m(pid=38435)\u001b[0m [5, 10000] loss:  0.463\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-318e61199324>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# You can change the number of GPUs per trial here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-e42d60fdfdd6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(num_samples, max_num_epochs, gpus_per_trial)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Run the search.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     result = tune.run(\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cifar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mresources_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gpu'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgpus_per_trial\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.8/envs/388/lib/python3.8/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0mtune_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVerbosity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV1_EXPERIMENT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0m_report_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_reporter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.8/envs/388/lib/python3.8/site-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_staging_grace_period\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_no_available_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.8/envs/388/lib/python3.8/site-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36m_process_events\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0;31m# TODO(ujvl): Consider combining get_next_available_trial and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0;31m#  fetch_result functionality so that we don't timeout on fetch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m             trial = self.trial_executor.get_next_available_trial(\n\u001b[0m\u001b[1;32m    523\u001b[0m                 timeout=timeout)  # blocking\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.8/envs/388/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\u001b[0m in \u001b[0;36mget_next_available_trial\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;31m# See https://github.com/ray-project/ray/issues/4211 for details.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mready\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffled_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.8/envs/388/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclient_mode_enabled\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_client_hook_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.8/envs/388/lib/python3.8/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_refs, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   1591\u001b[0m         \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1592\u001b[0m         \u001b[0mtimeout_milliseconds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m         ready_ids, remaining_ids = worker.core_worker.wait(\n\u001b[0m\u001b[1;32m   1594\u001b[0m             \u001b[0mobject_refs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1595\u001b[0m             \u001b[0mnum_returns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # You can change the number of GPUs per trial here.\n",
    "    main(num_samples=10, max_num_epochs=10, gpus_per_trial=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
